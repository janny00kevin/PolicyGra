%\documentclass[aspectratio=169]{beamer}
% use this if you don't want the bookmark tab to show up on the left
% by default, it's true
\documentclass[hyperref={bookmarks=false}]{beamer}	
%\documentclass[aspectratio=43]{beamer}
\usepackage{tikz}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
decorations.pathreplacing,decorations.pathmorphing,shapes,%
matrix,shapes.symbols}

\mode<presentation> {
	\usetheme{Madrid}
	% \usetheme{CambridgeUS}
}

\usepackage[ampersand]{easylist}
\usepackage{bookmark,booktabs,comment,graphicx,subcaption,epsfig,amsfonts,amsmath,amssymb,mathrsfs,color,enumerate,empheq,wrapfig,empheq,epstopdf}
\numberwithin{figure}{section}

\DeclareMathOperator*{\tr}{tr}
\DeclareMathOperator*{\blockdiag}{blockdiag}
\DeclareMathOperator*{\sgn}{sgn}

\usefonttheme{serif}	% make letters/symbols that should be boldfaced boldface

\input{Symbol_Shortcut.tex}

\title{MMSE Based Channel Estimator}
\author{Zhao-Jie, Luo}
\institute[NYCU] % Your institution as it will appear on the bottom of every slide, may be shorthand to save space
{
\textit{janny00kevin@gmail.com} % Your email address
\\
\medskip
Advisor: Professor Carrson C. Fung\\ 
\medskip
National Yang Ming Chiao Tung University \\ % Your institution for the title page
}
\date{May. 2 2024}

\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline} %\insertsectionhead
    \tableofcontents[currentsection]
  \end{frame}
}
\setbeamertemplate{frametitle continuation}{(\insertcontinuationcount)}

\begin{document}
\frame{\titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Reference}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
    \item Carrson C. Fung, and Dmytro Ivakhnenkov, "Model-Driven Neural Network Based MIMO Channel Estimator"
    \item M. Eisen, C. Zhang, L.F.O. Chamon, D.D. Lee and A. Ribeiro, ``Learning optimal power allocations in wireless systems,'' \emph{IEEE Trans. on Signal Processing}, vol. 67(10), pp. 2775-2790, May 2019.
    \item OpenAI Spinning Up introduction to RL Part 3: Intro to Policy Optimization %: \href{https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html}
    % \item S. Shi \emph{et al.}, ``MMSE optimization with per-base-station power constraints for network MIMO systems'', \emph{Proc. of the IEEE Intl. Conf. on Communications}, pp. 4106-4110, Bejing, China, May 2008
\end{enumerate}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Problem Formulation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Our objective is to minimize the expected mean square error:
\begin{align*}
    \min_{\hat{\h}} \mathbb{E}_{\y,\h}\left[\left\| \h-\hat{\h} \right\|^2_2 \right]\\
\end{align*}
and it can be written in epigraph form as:
\begin{align*}
    \min_{t,\h} {t}\\
    s.t. &\ \mathbb{E}_{\y,\h}\left[\left\| \h-\hat{\h} \right\|^2_2 \right] \leq t\\
\end{align*}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]{Primal-Dual Optimization Mehthod}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We use parameterize channel estimator so that $\hat{\h}=\phi(\mathbf{y};\boldsymbol{\theta})$, 
with $\boldsymbol{\theta}$ denoting the parameters of the neural network.
\\ \hspace*{\fill} \\
Then the Lagrangian function of (15) can be written as
\begin{equation*}
    \begin{split}
        \mathcal{L} \left(\hat{\h},t,\lambda\right)
        &= t + \lambda\left(\mathbb{E}_{\y,\h}\left[\left\| \h-\hat{\h} \right\|^2_2 \right] -t \right)\\
        &= t + \lambda\left(\mathbb{E}_{\y,\h}\left[\left\| \h-\phi(\mathbf{y};\boldsymbol{\theta}) \right\|^2_2 \right] -t \right)
    \end{split}
\end{equation*}
\framebreak

It is uncertain whether or not the duality gap equals zero.\\
However, the stationary point of $  \mathcal{L} \left(\hat{\h},t,\lambda\right)$
can be found via the KKT conditions by solving for the primal and dual
variables alternately using gradient descent and ascent, respectively:

\begin{equation*}
    \begin{split}
        \boldsymbol{\theta}_{k+1} &= \boldsymbol{\theta}_{k} - 
            \alpha_{\boldsymbol{\theta},k} \lambda_k \nabla_{\boldsymbol{\theta}_k}
            \mathbb{E}\left[\left\| \h-\phi(\mathbf{y};\boldsymbol{\theta}_k) \right\|^2_2 \right]\\
        t_{k+1} &= t_{k} - \alpha_{t,k}(1-\lambda_k)\\
        \lambda_{k+1} &= \left[ \lambda_k + \alpha_{\lambda,k}
            \left( \mathbb{E}\left[\left\| \h-\phi(\mathbf{y};\boldsymbol{\theta}_{k+1}) \right\|^2_2 \right] 
            -t_{k+1}\right)\right]_{+}
    \end{split}
\end{equation*}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]{Policy Gradient}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We have the policy gradient theorem:
\begin{align*}
    \nabla_{\boldsymbol{\theta}}\mathbb{E}_{\tau}[G(\tau)]
        =\mathbb{E}_{\tau}\left[\sum\limits_{t=0}^{T-1}G(\tau)
        \nabla_{\boldsymbol{\theta}}\log{\pi_{\boldsymbol{\theta}}
        (A_t|S_t)}\right]
\end{align*}
At each time step, $t=1,...,T-1$:
\begin{align*}
    \nabla_{\boldsymbol{\theta}}\mathbb{E}_{\tau}[G(\tau)]
        =\mathbb{E}_{\tau}\left[G(\tau)
        \nabla_{\boldsymbol{\theta}}\log{\pi_{\boldsymbol{\theta}}
        (A_t|S_t)}\right]
\end{align*}
And we can estimate the policy gradient with sample mean:
\begin{align*}
    \widehat{\nabla_{\boldsymbol{\theta}}}\mathbb{E}_{\tau}[G(\tau)]
        =\frac{1}{\left| \mathcal{D}  \right|} \sum\limits_{\mathcal{D}}
        G(\tau)\nabla_{\boldsymbol{\theta}}\log{\pi_{\boldsymbol{\theta}}
        (A_t|S_t)}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\framebreak

Our goal is to minimize the mean square error, by substituting 
$\mathbb{E}_{\tau}[G(\tau)]$, $\pi_{\boldsymbol{\theta}}(A_t|S_t)$ with 
$\mathbb{E}_{\y,\h}\left[\left\| \h-\phi(\mathbf{y};\boldsymbol{\theta}) \right\|^2_2 \right]$,
and $\pi_{\boldsymbol{\theta}}(\hat{\h}|\y)$.\\
Thus, we obtain the estimated policy gradient for our problem:
\begin{align*}
    \widehat{\nabla_{\boldsymbol{\theta}}}
            \mathbb{E}_{\y,\h}\left[\left\| \h-\phi(\mathbf{y};\boldsymbol{\theta}) \right\|^2_2 \right]
            = \frac{1}{\left| \mathcal{D}  \right|} \sum\limits_{\mathcal{D}}
            \left\| \h-\widehat{\phi}(\mathbf{y};\boldsymbol{\theta}) \right\|^2_2
            \nabla_{\boldsymbol{\theta}}
            \log \pi_{\boldsymbol{\theta}}
            \left( \hat{\h}|\y \right),
\end{align*}
where $\hat{\phi}(\mathbf{y};\boldsymbol{\theta}) = 
\hat{\h}$ is the sampled output of the policy. 

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experiment Diagram}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{tikzpicture}[start chain=going below, node distance=50pt]
    \node[] (a) {};
    \node[draw, right=3.5cm of a] (MLP) {MLP};
    \node[draw, right=2cm of MLP] (sample) {sample};
    \draw [->] (MLP) -- node[name=N, align=center,font=\scriptsize] 
            {$\mathcal{N}(\mu_i,\sigma_i)$\\
            \fontsize{6}{6}\selectfont$i=1,...,
            2n_Rn_T$} (sample);
    \node[draw, above=4cm of N] (batch) {batch};
    \node[draw, below of=N, align=center,font=\small]   (env) 
            {Environment \\ \fontsize{9}{12}\selectfont $\Y=\H\X+\W$};
    \draw [->] (sample) --(10,0)--(10,4.7)-- (batch);
    \draw [->] (env) --(1.8,-1.75)--(1.8,4.7)-- (batch);
    \draw [->] (env) --(5.75,-2.5)--(1,-2.5)--(1,5.25)--(5.75,5.25)-- (batch);
    \draw [-]  (1,-2.5)--node[name=H,align=center] {$\h\quad$} (1,5.25);
    \draw [->] (1.8,0) --node[name=Y,align=center,font=\footnotesize]
            {$\y \in \mathbb{R}^{2n_RT}$\\} (MLP);
    \draw [->] (batch) --(5.75,4)--(2.75,4)--(2.75,1)--(4.2,1)-- (MLP);
    \draw [-,] (sample) --node[name=Hhat,align=center,font=\footnotesize] 
            {$\hat{\h}\in\mathbb{R} ^{2n_Tn_R}$\\} (10,0);
    \node [font=\tiny] at (6.15,3) {$\lambda_{k+1} = \left[ \lambda_k + \alpha_{\lambda,k}
            \left( \frac{1}{\left| \mathcal{D}  \right|} \sum\limits_{\mathcal{D}} \left\| \h-\widehat{\phi}(\mathbf{y};\boldsymbol{\theta}_{k}) \right\|^2_2
            -t_{k}\right)\right]_{+}$};
    \node [font=\tiny] at (5.65,2.5) {$\boldsymbol{\theta}_{k+1} = \boldsymbol{\theta}_{k} - 
            \alpha_{\boldsymbol{\theta},k} \lambda_k \widehat{\nabla}_{\boldsymbol{\theta}_k}
            \mathbb{E}\left[\left\| \h-\phi(\mathbf{y};\boldsymbol{\theta}_k) \right\|^2_2 \right]$};
    \node [font=\tiny] at (4.6,2) {$t_{k+1} = t_{k} - \alpha_{t,k}(1-\lambda_{k+1})$};
    \node [align=left] at (8.3,-1.75) 
            {\parbox[b]{1.7cm}{\fontsize{6}{6}\selectfont 
                $\Y \in \mathbb{C}^{n_R \times T}$\\
                $\H \in \mathbb{C}^{n_R \times n_T}$\\
                $\X \in \mathbb{C}^{n_T \times T}$\\
                $\W \in \mathbb{C}^{n_R \times T}$}
            };
    % \node at (0,0) {\parbox[b]{3cm}{\fontsize{24}{28}\selectfont Large Text\\ \fontsize{12}{14}\selectfont Normal Text}};
\end{tikzpicture}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]{Simulation Result}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
"Epoch" and "iteration" in the following 3 pages:\\
\begin{enumerate}
    \item The process consisting of data collection followed
        by model updating is called one "iteration".\\
    \item Performing the processes above for a number of 
        iterations, which means that updating the model for a 
        number of iterations, is called one "epoch" here.\\
    \item After Performing the whole process number of 
        epoch times, taking the average of these loss curve 
        and then plot the figures in the following 3 pages. 
\end{enumerate}


\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h]
    \includegraphics[width=0.7\textwidth]{Figure_1.png}
    \caption{hidden layer sizes: [64,32], $\mu_H=5, \sigma_H=0.2, \mu_W=0, \sigma_W=0.1 $}
\end{figure}

\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \includegraphics[width=0.7\textwidth]{Figure_2.png}
    \caption{increase the iterations from 2000 to 4000}
\end{figure}

\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{itr_500/Figure_1.png}
    %   \caption{Image 1}
    %   \label{fig:image1}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{itr_500/Figure_2.png}
    %   \caption{Image 2}
    %   \label{fig:image2}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{itr_500/Figure_3.png}
    %   \caption{Image 3}
    %   \label{fig:image3}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{itr_500/Figure_4.png}
    %   \caption{Image 4}
    %   \label{fig:image4}
    \end{subfigure}
    \caption{Number of trajectories from 1 to 1000}
    % \label{fig:four_images}
  \end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[allowframebreaks]{Simulation Configuration}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \begin{tabular}{|c|c|}
        \hline
        parameters   &values \\
        \hline \hline
        [$n_R,n_T$]                     &[1,1] or [4,36]        \\ \hline
        $T$                             &$n_T \times 1$         \\ \hline
        $\mu_H, \sigma_H$               &0, 1          \\ \hline
        Hidden layer size               &[64,32] or [512,512]   \\ \hline
        Length of each trajectory       &1                      \\ \hline
        Batch size = $|\mathcal{D}|$    &10,000         \\ \hline
        Number of batches               &100            \\ \hline
        Trainning dataset               &1,000,000      \\ \hline
        Validation dataset              &2,000          \\ \hline
        Epoch                           &100 $\sim$ 2,000   \\ \hline
        Learning rate                   &1e-3 $\sim$ 1e-5  \\ \hline
    \end{tabular}
    \end{table}

\framebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}
    \item Let the loss be Normalized Mean Squared Error (NMSE):
        $$NMSE=\frac{1}{N} \sum\limits_{n=1}^N
        \frac{\left\| \h_n-\phi(\mathbf{y}_n;\boldsymbol{\theta}_k) \right\|^2_2}
        {\left\| \h_n \right\|^2_2}$$
        Here, $N$ represents the size of the training or validation dataset, 
        implying that the Normalized Mean Squared Error (NMSE) is calculated for each epoch.
    \vspace{12pt}
    \item Let step size $\alpha_{t,k}$ and $\alpha_{\lambda,k}$ to be ${\alpha_{t,0}}/{k}$
        and ${\alpha_{\lambda,0}}/{k}$, where $k$ is the iteration index.
 
\end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Simulation Result: $n_R, n_T = 1, 1$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h!]
    % \centering
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/lr0.001_[2, 64, 32, 4]_ep100.png}
    %   \caption{lr: 1e-3, epoch: 100}
    %   \label{fig:image1}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/lr0.001_[2, 64, 32, 4]_ep500.png}
    %   \caption{lr: 1e-3, epoch: 100}
    %   \label{fig:image2}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/figure_1_0.0001_[2, 64, 32, 4]_ep100.png}
    %   \caption{Image 3}
    %   \label{fig:image3}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/figure_1_0.0001_[2, 64, 32, 4]_ep1000.png}
    %   \caption{Image 4}
    %   \label{fig:image4}
    \end{subfigure}
    \caption{$n_R, n_T$ = [1,1], hidden layer size = [64,32]
        (a)(b) lr: 1e-3, epoch from 100 to 500
        (c)(d) lr: 1e-4, epoch from 100 to 1000}
    % \label{fig:four_images}
  \end{figure}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Simulation Result: $n_R, n_T = 4, 36$}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[h!]
    % \centering
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/lr2e-05_[288, 512, 256, 256, 576]_ep500.png}
    %   \caption{lr: 1e-3, epoch: 100}
    %   \label{fig:image1}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/lr2e-05_[288, 512, 512, 576]_ep500.png}
    %   \caption{lr: 1e-3, epoch: 100}
    %   \label{fig:image2}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/figure_1_0.0001_[288, 64, 32, 576]_ep500.png}
    %   \caption{Image 3}
    %   \label{fig:image3}
    \end{subfigure}
    \begin{subfigure}[b]{0.33\linewidth}
      \includegraphics[width=\linewidth]{240502/figure_1_lr0.0001_[288, 64, 32, 576]_ep2000.png}
    %   \caption{Image 4}
    %   \label{fig:image4}
    \end{subfigure}
    \caption{$n_R, n_T$ = [4,36] 
        (a)(b) hidden layer size = [512,256,256], [512,512] lr: 2e-5, epoch: 500
        (c)(d) hidden layer size = [64,32] lr: 1e-4, epoch from 500 to 1000}
    \label{fig:4_36}
\end{figure}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conclution $\&$ Problems}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
  \item In Rayleigh fading channel, if we choose an appropriate learning 
    rate, the loss will converge without raising up or spike and there's 
    no overfitting problem in at least 2000 epochs.
  \vspace{12pt}
  \item However, Professor noticed that the NMSE in linear scale approaches
    1 but not 0. This could be attributed to the variance generated by the 
    MLP converging to 0. Consequently, $\hat{\h}$ sampled from this 
    distribution also converge to $\0$.
  \item Professor suggested first checking $\hat{\h}$ to identify any 
    potential issues. Alternatively, removing the sampling process to see 
    if there is any improvment.

\end{itemize}
\end{frame}

\end{document}
